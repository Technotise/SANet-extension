{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4342ba61-7ce0-4b0d-a4b9-fd25dcc93a54",
   "metadata": {},
   "source": [
    "<h2>Vorbereitung der Umgebung</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac6d5ec-530e-49f2-9496-8fff87cb35f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.24.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score rouge sacrebleu nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13598f9d-79d6-4c83-bcff-3507e5ed1f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from rouge import Rouge\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu.metrics import BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "sys.path.append('..')  # Gehe eine Ebene höher\n",
    "from net_lstm import LSTM_seq\n",
    "from config import num_classes, hidden_size, batch_size, epochs, learning_rate\n",
    "import logging\n",
    "\n",
    "# Sicherstellen, dass die relevanten Dateien vorhanden sind\n",
    "model_path = \"trained_lstm_model.pth\"\n",
    "test_data_path = \"processed_test_data.pt\"\n",
    "test_vocab_path = \"test_vocab.json\"\n",
    "train_vocab_path = \"train_vocab.json\"\n",
    "\n",
    "# Prüfen, ob CUDA verfügbar ist\n",
    "device = torch.device(\"cuda:1\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924cee11-349a-4b1d-84bc-8be6d6d8a13b",
   "metadata": {},
   "source": [
    "<h2>Laden der Daten und Modelle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a4c14a-4092-4f2d-9d1f-9fb1b74d29a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell erfolgreich geladen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8997/2005671800.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"trained_lstm_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "# Modell initialisieren\n",
    "model = LSTM_seq(max_seq=52, input_size=10, hidden_size=512, class_num=1814).to(device)\n",
    "\n",
    "# Modell laden\n",
    "model.load_state_dict(torch.load(\"trained_lstm_model.pth\"))\n",
    "model.eval()  # Setze das Modell in den Evaluierungsmodus\n",
    "\n",
    "print(\"Modell erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7bd4c-906b-4d72-9ccb-b393e3d63c09",
   "metadata": {},
   "source": [
    "<h2>Testdaten laden</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26364261-793f-45a6-b411-ef6b9985ee4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Testdaten laden\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(test_data_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Outputs und Labels extrahieren\u001b[39;00m\n\u001b[1;32m      5\u001b[0m test_outputs \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Eingabe (Video-Features)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Testdaten laden\n",
    "test_data = torch.load(test_data_path)\n",
    "\n",
    "# Outputs und Labels extrahieren\n",
    "test_outputs = test_data[\"outputs\"].to(device)  # Eingabe (Video-Features)\n",
    "test_labels = test_data[\"labels\"].to(device)  # Zielsequenzen\n",
    "\n",
    "print(f\"Testdaten geladen: Outputs Shape: {test_outputs.shape}, Labels Shape: {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c9e1f-476d-41ad-a252-f9ec06362870",
   "metadata": {},
   "source": [
    "<h2>Validierung</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccc5f95-ceb0-445d-9e27-774316073a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6048\n"
     ]
    }
   ],
   "source": [
    "# Initialisiere Variablen für die Accuracy\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "\n",
    "# Berechnung der Accuracy\n",
    "with torch.no_grad():\n",
    "    for i in range(test_outputs.size(0)):\n",
    "        # Hole die Eingaben und Labels\n",
    "        video_input = test_outputs[i].unsqueeze(0).to(device)  # Eingabedaten auf cuda:1\n",
    "        label = test_labels[i].unsqueeze(0).to(device)  # Labels auf cuda:1\n",
    "\n",
    "        # Modellvorhersage\n",
    "        output = model(video_input, label)\n",
    "        prediction = torch.argmax(output, dim=2).squeeze(0).cpu().numpy()\n",
    "        ground_truth = label.squeeze(0).cpu().numpy()\n",
    "\n",
    "        # Korrekte Tokens zählen\n",
    "        for pred, true in zip(prediction, ground_truth):\n",
    "            if pred == true:\n",
    "                correct_tokens += 1\n",
    "            total_tokens += 1\n",
    "\n",
    "# Berechne die Gesamtaccuracy\n",
    "validation_accuracy = correct_tokens / total_tokens\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44510268-7df7-4861-94b8-4c475b057697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: 0.7317\n",
      "ROUGE-2: 0.7250\n",
      "ROUGE-L: 0.7317\n",
      "BLEU Score (SacreBLEU): 57.2785\n",
      "BLEU Scores (nltk):\n",
      "BLEU-1: 0.5769\n",
      "BLEU-2: 0.5728\n",
      "BLEU-3: 0.5717\n",
      "BLEU-4: 0.5641\n"
     ]
    }
   ],
   "source": [
    "# Logging für sacrebleu auf ERROR setzen\n",
    "logging.getLogger(\"sacrebleu\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialisiere ROUGE und BLEU Scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "bleu = BLEU()\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "# Erstelle DataLoader für Testdaten\n",
    "test_dataset = torch.utils.data.TensorDataset(test_outputs, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Lade das Vokabular (falls nötig)\n",
    "with open(test_vocab_path, \"r\") as f:\n",
    "    test_vocab = json.load(f)\n",
    "    \n",
    "# Konvertiere Vokabular in ein Wörterbuch, falls es eine Liste ist\n",
    "if isinstance(test_vocab, list):\n",
    "    test_vocab = {idx: word for idx, word in enumerate(test_vocab)}\n",
    "\n",
    "# Dekodierfunktion\n",
    "def decode_sequence(sequence, vocab):\n",
    "    # Vokabular umkehren: ID -> Wort\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return \" \".join([inv_vocab.get(token, \"<UNK>\") for token in sequence])\n",
    "\n",
    "# Wahre Labels und Vorhersagen sammeln\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "        # Daten auf das richtige Gerät übertragen\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Vorhersagen generieren\n",
    "        output = model(batch_features, batch_labels)\n",
    "        predictions = output.argmax(dim=-1).cpu().numpy()  # Vorhersage\n",
    "\n",
    "        # Labels und Vorhersagen sammeln\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "# ROUGE- und BLEU-Bewertung\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "bleu_scores = []\n",
    "bleu_1_scores = []\n",
    "bleu_2_scores = []\n",
    "bleu_3_scores = []\n",
    "bleu_4_scores = []\n",
    "\n",
    "for label, prediction in zip(all_labels, all_predictions):\n",
    "    # Dekodieren\n",
    "    label_text = decode_sequence(label, test_vocab)\n",
    "    prediction_text = decode_sequence(prediction, test_vocab)\n",
    "\n",
    "    # ROUGE-Bewertung\n",
    "    rouge_result = rouge.score(label_text, prediction_text)\n",
    "    for metric in rouge_scores.keys():\n",
    "        rouge_scores[metric].append(rouge_result[metric].fmeasure)\n",
    "\n",
    "    # BLEU-Bewertung (SacreBLEU und nltk)\n",
    "    bleu_scores.append(bleu.sentence_score(prediction_text, [label_text]).score)\n",
    "\n",
    "    label_tokens = label_text.split()\n",
    "    prediction_tokens = prediction_text.split()\n",
    "    bleu_1 = sentence_bleu([label_tokens], prediction_tokens, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "    bleu_2 = sentence_bleu([label_tokens], prediction_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
    "    bleu_3 = sentence_bleu([label_tokens], prediction_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth)\n",
    "    bleu_4 = sentence_bleu([label_tokens], prediction_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "\n",
    "    bleu_1_scores.append(bleu_1)\n",
    "    bleu_2_scores.append(bleu_2)\n",
    "    bleu_3_scores.append(bleu_3)\n",
    "    bleu_4_scores.append(bleu_4)\n",
    "\n",
    "# Durchschnittswerte berechnen\n",
    "average_rouge = {metric: sum(scores) / len(scores) for metric, scores in rouge_scores.items()}\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_bleu_1 = sum(bleu_1_scores) / len(bleu_1_scores)\n",
    "average_bleu_2 = sum(bleu_2_scores) / len(bleu_2_scores)\n",
    "average_bleu_3 = sum(bleu_3_scores) / len(bleu_3_scores)\n",
    "average_bleu_4 = sum(bleu_4_scores) / len(bleu_4_scores)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {average_rouge['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {average_rouge['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {average_rouge['rougeL']:.4f}\")\n",
    "print(f\"BLEU Score (SacreBLEU): {average_bleu:.4f}\")\n",
    "print(\"BLEU Scores (nltk):\")\n",
    "print(f\"BLEU-1: {average_bleu_1:.4f}\")\n",
    "print(f\"BLEU-2: {average_bleu_2:.4f}\")\n",
    "print(f\"BLEU-3: {average_bleu_3:.4f}\")\n",
    "print(f\"BLEU-4: {average_bleu_4:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40f24f-505b-4ddd-87f7-41ec9aa0ebbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
